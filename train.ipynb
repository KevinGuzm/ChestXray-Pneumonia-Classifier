{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 73450,
     "databundleVersionId": 8105846,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30787,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "source": "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\n\n# Configuración del dispositivo (CPU o GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Directorios\ntrain_dir = '/kaggle/input/pneumonia-sai3/pneumonia-kaggle/train/'\ntest_dir = '/kaggle/input/pneumonia-sai3/pneumonia-kaggle/test/'\n\n# Parámetros\nIMG_SIZE = (150, 150)\nBATCH_SIZE = 32\nNUM_EPOCHS = 30\nLEARNING_RATE = 0.001\n\n# Transformaciones\ntransform = transforms.Compose([\n    transforms.Resize(IMG_SIZE),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\n# Cargar datasets\ntrain_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_data, val_data = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n\n# Dataset personalizado para imágenes no clasificadas\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, image_dir, transform=None):\n        self.image_dir = image_dir\n        self.image_filenames = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, self.image_filenames[idx]\n\n# Dataset de prueba\ntest_dataset = TestDataset(image_dir=test_dir, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n# Clase del modelo CNN\nclass PneumoniaCNN(nn.Module):\n    def __init__(self):\n        super(PneumoniaCNN, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        self._initialize_fc_input()\n\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(self.fc_input_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def _initialize_fc_input(self):\n        dummy_input = torch.zeros(1, 3, *IMG_SIZE)\n        output = self.conv_layers(dummy_input)\n        self.fc_input_dim = output.view(-1).shape[0]\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        return x\n\n# Inicialización del modelo\nmodel = PneumoniaCNN().to(device)\n\n# Función de pérdida y optimizador\ncriterion = nn.BCELoss()\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-18T22:20:58.070231Z",
     "iopub.execute_input": "2024-11-18T22:20:58.070651Z",
     "iopub.status.idle": "2024-11-18T22:21:07.212643Z",
     "shell.execute_reply.started": "2024-11-18T22:20:58.070614Z",
     "shell.execute_reply": "2024-11-18T22:21:07.211926Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Using device: cuda\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Evaluación del modelo (con AUROC)\ndef evaluate_model_auroc(model, loader):\n    model.eval()\n    all_labels = []\n    all_predictions = []\n\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device).float()\n            outputs = model(images).squeeze()\n            all_predictions.extend(outputs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calcular AUROC\n    auroc = roc_auc_score(all_labels, all_predictions)\n    return auroc\n\n# Entrenamiento del modelo con AUROC\ndef train_model_auroc(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        all_labels = []\n        all_predictions = []\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device).float()\n\n            optimizer.zero_grad()\n            outputs = model(images).squeeze()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            all_predictions.extend(outputs.cpu().detach().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n        # Cálculo de AUROC en entrenamiento\n        train_auroc = roc_auc_score(all_labels, all_predictions)\n\n        # Evaluación en validación\n        val_auroc = evaluate_model_auroc(model, val_loader)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, \"\n              f\"Train AUROC: {train_auroc:.4f}, Val AUROC: {val_auroc:.4f}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-18T22:21:13.076150Z",
     "iopub.execute_input": "2024-11-18T22:21:13.076813Z",
     "iopub.status.idle": "2024-11-18T22:21:13.084973Z",
     "shell.execute_reply.started": "2024-11-18T22:21:13.076780Z",
     "shell.execute_reply": "2024-11-18T22:21:13.083914Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Entrenamiento\ntrain_model_auroc(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS)\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-18T22:21:16.522417Z",
     "iopub.execute_input": "2024-11-18T22:21:16.523169Z",
     "iopub.status.idle": "2024-11-18T23:01:06.844066Z",
     "shell.execute_reply.started": "2024-11-18T22:21:16.523124Z",
     "shell.execute_reply": "2024-11-18T23:01:06.843162Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/30, Loss: 0.9604, Train AUROC: 0.8960, Val AUROC: 0.9887\nEpoch 2/30, Loss: 0.1435, Train AUROC: 0.9832, Val AUROC: 0.9932\nEpoch 3/30, Loss: 0.1470, Train AUROC: 0.9823, Val AUROC: 0.9957\nEpoch 4/30, Loss: 0.1458, Train AUROC: 0.9825, Val AUROC: 0.9946\nEpoch 5/30, Loss: 0.1355, Train AUROC: 0.9857, Val AUROC: 0.9964\nEpoch 6/30, Loss: 0.1111, Train AUROC: 0.9897, Val AUROC: 0.9967\nEpoch 7/30, Loss: 0.0975, Train AUROC: 0.9920, Val AUROC: 0.9953\nEpoch 8/30, Loss: 0.1098, Train AUROC: 0.9895, Val AUROC: 0.9977\nEpoch 9/30, Loss: 0.0964, Train AUROC: 0.9925, Val AUROC: 0.9942\nEpoch 10/30, Loss: 0.1040, Train AUROC: 0.9912, Val AUROC: 0.9975\nEpoch 11/30, Loss: 0.0939, Train AUROC: 0.9923, Val AUROC: 0.9976\nEpoch 12/30, Loss: 0.0880, Train AUROC: 0.9936, Val AUROC: 0.9974\nEpoch 13/30, Loss: 0.0891, Train AUROC: 0.9934, Val AUROC: 0.9972\nEpoch 14/30, Loss: 0.0893, Train AUROC: 0.9931, Val AUROC: 0.9983\nEpoch 15/30, Loss: 0.0878, Train AUROC: 0.9937, Val AUROC: 0.9977\nEpoch 16/30, Loss: 0.0851, Train AUROC: 0.9942, Val AUROC: 0.9982\nEpoch 17/30, Loss: 0.1000, Train AUROC: 0.9916, Val AUROC: 0.9986\nEpoch 18/30, Loss: 0.0756, Train AUROC: 0.9953, Val AUROC: 0.9978\nEpoch 19/30, Loss: 0.0811, Train AUROC: 0.9948, Val AUROC: 0.9979\nEpoch 20/30, Loss: 0.0761, Train AUROC: 0.9953, Val AUROC: 0.9975\nEpoch 21/30, Loss: 0.0629, Train AUROC: 0.9967, Val AUROC: 0.9980\nEpoch 22/30, Loss: 0.0773, Train AUROC: 0.9953, Val AUROC: 0.9978\nEpoch 23/30, Loss: 0.0770, Train AUROC: 0.9953, Val AUROC: 0.9990\nEpoch 24/30, Loss: 0.0729, Train AUROC: 0.9957, Val AUROC: 0.9987\nEpoch 25/30, Loss: 0.0738, Train AUROC: 0.9955, Val AUROC: 0.9976\nEpoch 26/30, Loss: 0.0674, Train AUROC: 0.9962, Val AUROC: 0.9984\nEpoch 27/30, Loss: 0.0619, Train AUROC: 0.9966, Val AUROC: 0.9990\nEpoch 28/30, Loss: 0.0645, Train AUROC: 0.9966, Val AUROC: 0.9976\nEpoch 29/30, Loss: 0.0691, Train AUROC: 0.9960, Val AUROC: 0.9977\nEpoch 30/30, Loss: 0.0669, Train AUROC: 0.9964, Val AUROC: 0.9983\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Predicción en el conjunto de prueba\nmodel.eval()\ntest_results = []\n\nwith torch.no_grad():\n    for images, filenames in test_loader:\n        images = images.to(device)\n        outputs = model(images).squeeze()\n        probability = outputs.item()\n        test_results.append({'Id': filenames[0], 'Category': probability})\n\n# Guardar las predicciones en un archivo CSV\nresults_df = pd.DataFrame(test_results)\nresults_df.to_csv('predictions0.csv', index=False)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-18T23:01:19.895143Z",
     "iopub.execute_input": "2024-11-18T23:01:19.896000Z",
     "iopub.status.idle": "2024-11-18T23:01:33.045824Z",
     "shell.execute_reply.started": "2024-11-18T23:01:19.895948Z",
     "shell.execute_reply": "2024-11-18T23:01:33.045095Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Guardar solo los pesos\ntorch.save(model.state_dict(), 'model_weights.pth')\n\n# Guardar el modelo completo\ntorch.save(model, 'model_complete.pth')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-18T23:05:34.371944Z",
     "iopub.execute_input": "2024-11-18T23:05:34.372541Z",
     "iopub.status.idle": "2024-11-18T23:05:34.441710Z",
     "shell.execute_reply.started": "2024-11-18T23:05:34.372508Z",
     "shell.execute_reply": "2024-11-18T23:05:34.440765Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  }
 ]
}
